{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a8e8dc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02036372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/charaf/DeepLearning/Project/Deep_Learning\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8fbae",
   "metadata": {},
   "source": [
    "## Read data/paradox.tsv file as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954254a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>neutral1</th>\n",
       "      <th>neutral2</th>\n",
       "      <th>neutral3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he had steel balls too !</td>\n",
       "      <td>he was brave too!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dude should have been taken to api , he would ...</td>\n",
       "      <td>It would have been good if he went to api. He ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im not gonna sell the fucking picture , i just...</td>\n",
       "      <td>I'm not gonna sell the picture, i just want to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the garbage that is being created by cnn and o...</td>\n",
       "      <td>the news that is being created by cnn and othe...</td>\n",
       "      <td>The news that is being created by cnn and othe...</td>\n",
       "      <td>the garbage that is being created by cnn and o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the reason they dont exist is because neither ...</td>\n",
       "      <td>The reason they don't exist is because neither...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               toxic  \\\n",
       "0                           he had steel balls too !   \n",
       "1  dude should have been taken to api , he would ...   \n",
       "2  im not gonna sell the fucking picture , i just...   \n",
       "3  the garbage that is being created by cnn and o...   \n",
       "4  the reason they dont exist is because neither ...   \n",
       "\n",
       "                                            neutral1  \\\n",
       "0                                  he was brave too!   \n",
       "1  It would have been good if he went to api. He ...   \n",
       "2  I'm not gonna sell the picture, i just want to...   \n",
       "3  the news that is being created by cnn and othe...   \n",
       "4  The reason they don't exist is because neither...   \n",
       "\n",
       "                                            neutral2  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  The news that is being created by cnn and othe...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            neutral3  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  the garbage that is being created by cnn and o...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, 'data')\n",
    "df = pd.read_csv(data_dir +'/paradetox.tsv', sep='\\t')\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a77667",
   "metadata": {},
   "source": [
    "### Kept for every toxic sentence just one non-toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d99cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ --------------------\n",
      "accelerate               1.6.0\n",
      "aiohappyeyeballs         2.6.1\n",
      "aiohttp                  3.11.18\n",
      "aiosignal                1.3.2\n",
      "asttokens                3.0.0\n",
      "async-timeout            5.0.1\n",
      "attrs                    21.2.0\n",
      "Automat                  20.2.0\n",
      "Babel                    2.8.0\n",
      "bcrypt                   3.2.0\n",
      "beautifulsoup4           4.10.0\n",
      "blinker                  1.4\n",
      "breezy                   3.2.1\n",
      "brz-debian               2.8.51\n",
      "certifi                  2020.6.20\n",
      "Chameleon                3.8.1\n",
      "chardet                  4.0.0\n",
      "charset-normalizer       3.4.2\n",
      "click                    8.0.3\n",
      "cloud-init               24.4.1\n",
      "colorama                 0.4.4\n",
      "comm                     0.2.2\n",
      "command-not-found        0.3\n",
      "configobj                5.0.6\n",
      "constantly               15.1.0\n",
      "cryptography             3.4.8\n",
      "datasets                 3.5.1\n",
      "dbus-python              1.2.18\n",
      "debmutate                0.48\n",
      "debugpy                  1.8.14\n",
      "decorator                5.2.1\n",
      "Deprecated               1.2.13\n",
      "devscripts               2.22.1ubuntu1\n",
      "dill                     0.3.8\n",
      "distro                   1.7.0\n",
      "distro-info              1.1+ubuntu0.2\n",
      "docutils                 0.17.1\n",
      "dulwich                  0.20.31\n",
      "exceptiongroup           1.2.2\n",
      "executing                2.2.0\n",
      "fastbencode              0.0.5\n",
      "fastimport               0.9.14\n",
      "filelock                 3.18.0\n",
      "frozenlist               1.6.0\n",
      "fsspec                   2025.3.0\n",
      "gpg                      1.16.0\n",
      "html5lib                 1.1\n",
      "httplib2                 0.20.2\n",
      "huggingface-hub          0.30.2\n",
      "hyperlink                21.0.0\n",
      "idna                     3.3\n",
      "importlib-metadata       4.6.4\n",
      "incremental              21.3.0\n",
      "ipykernel                6.29.5\n",
      "ipython                  8.36.0\n",
      "jedi                     0.19.2\n",
      "jeepney                  0.7.1\n",
      "Jinja2                   3.1.2\n",
      "jsonpatch                1.32\n",
      "jsonpointer              2.0\n",
      "jsonschema               3.2.0\n",
      "jupyter_client           8.6.3\n",
      "jupyter_core             5.7.2\n",
      "keyring                  23.5.0\n",
      "launchpadlib             1.10.16\n",
      "lazr.restfulclient       0.14.4\n",
      "lazr.uri                 1.0.6\n",
      "lptools                  0.2.0\n",
      "lxml                     4.8.0\n",
      "Markdown                 3.3.6\n",
      "markdown-it-py           3.0.0\n",
      "MarkupSafe               2.0.1\n",
      "matplotlib-inline        0.1.7\n",
      "mdurl                    0.1.2\n",
      "merge3                   0.0.8\n",
      "more-itertools           8.10.0\n",
      "mpmath                   1.3.0\n",
      "multidict                6.4.3\n",
      "multiprocess             0.70.16\n",
      "nest-asyncio             1.6.0\n",
      "netifaces                0.11.0\n",
      "networkx                 3.4.2\n",
      "numpy                    2.2.5\n",
      "nvidia-cublas-cu12       12.6.4.1\n",
      "nvidia-cuda-cupti-cu12   12.6.80\n",
      "nvidia-cuda-nvrtc-cu12   12.6.77\n",
      "nvidia-cuda-runtime-cu12 12.6.77\n",
      "nvidia-cudnn-cu12        9.5.1.17\n",
      "nvidia-cufft-cu12        11.3.0.4\n",
      "nvidia-cufile-cu12       1.11.1.6\n",
      "nvidia-curand-cu12       10.3.7.77\n",
      "nvidia-cusolver-cu12     11.7.1.2\n",
      "nvidia-cusparse-cu12     12.5.4.2\n",
      "nvidia-cusparselt-cu12   0.6.3\n",
      "nvidia-nccl-cu12         2.26.2\n",
      "nvidia-nvjitlink-cu12    12.6.85\n",
      "nvidia-nvtx-cu12         12.6.77\n",
      "oauthlib                 3.2.0\n",
      "olefile                  0.46\n",
      "packaging                21.3\n",
      "pandas                   2.2.3\n",
      "parso                    0.8.4\n",
      "patiencediff             0.2.1\n",
      "peft                     0.15.2\n",
      "pexpect                  4.8.0\n",
      "Pillow                   9.0.1\n",
      "pip                      22.0.2\n",
      "platformdirs             4.3.7\n",
      "prompt_toolkit           3.0.51\n",
      "propcache                0.3.1\n",
      "protobuf                 6.30.2\n",
      "psutil                   7.0.0\n",
      "psycopg2                 2.9.2\n",
      "ptyprocess               0.7.0\n",
      "pure_eval                0.2.3\n",
      "py3dns                   3.2.1\n",
      "pyarrow                  20.0.0\n",
      "pyasn1                   0.4.8\n",
      "pyasn1-modules           0.2.1\n",
      "pycurl                   7.44.1\n",
      "PyGithub                 1.55\n",
      "Pygments                 2.19.1\n",
      "PyGObject                3.42.1\n",
      "PyHamcrest               2.0.2\n",
      "PyJWT                    2.3.0\n",
      "PyNaCl                   1.5.0\n",
      "pyOpenSSL                21.0.0\n",
      "pyparsing                2.4.7\n",
      "pyrsistent               0.18.1\n",
      "pyserial                 3.5\n",
      "PySimpleSOAP             1.16.2\n",
      "python-apt               2.4.0+ubuntu4\n",
      "python-dateutil          2.9.0.post0\n",
      "python-debian            0.1.43+ubuntu1.1\n",
      "python-debianbts         3.2.0\n",
      "python-gitlab            2.10.1\n",
      "python-magic             0.4.24\n",
      "python-tr                0.1.2\n",
      "pytz                     2022.1\n",
      "pyxdg                    0.27\n",
      "PyYAML                   5.4.1\n",
      "pyzmq                    26.4.0\n",
      "regex                    2024.11.6\n",
      "reportbug                11.4.1ubuntu1\n",
      "requests                 2.32.3\n",
      "requests-toolbelt        0.9.1\n",
      "rich                     14.0.0\n",
      "roman                    3.3\n",
      "safetensors              0.5.3\n",
      "SecretStorage            3.3.1\n",
      "semver                   2.10.2\n",
      "sentencepiece            0.2.0\n",
      "service-identity         18.1.0\n",
      "setuptools               59.6.0\n",
      "six                      1.16.0\n",
      "sos                      4.8.2\n",
      "soupsieve                2.3.1\n",
      "ssh-import-id            5.11\n",
      "stack-data               0.6.3\n",
      "sympy                    1.14.0\n",
      "systemd-python           234\n",
      "tokenizers               0.21.1\n",
      "toml                     0.10.2\n",
      "tomlkit                  0.9.2\n",
      "torch                    2.7.0\n",
      "tornado                  6.4.2\n",
      "tqdm                     4.67.1\n",
      "traitlets                5.14.3\n",
      "transformers             4.51.3\n",
      "triton                   3.3.0\n",
      "trl                      0.17.0\n",
      "Twisted                  22.1.0\n",
      "typing_extensions        4.13.2\n",
      "tzdata                   2025.2\n",
      "ubuntu-dev-tools         0.201+ubuntu2.22.4.4\n",
      "ubuntu-drivers-common    0.0.0\n",
      "ubuntu-pro-client        8001\n",
      "ufw                      0.36.1\n",
      "unidiff                  0.5.5\n",
      "upstream-ontologist      0.1.24\n",
      "urllib3                  1.26.5\n",
      "wadllib                  1.3.6\n",
      "wcwidth                  0.2.13\n",
      "webencodings             0.5.1\n",
      "wheel                    0.37.1\n",
      "wrapt                    1.13.3\n",
      "xdg                      5\n",
      "xkit                     0.0.0\n",
      "xxhash                   3.5.0\n",
      "yarl                     1.20.0\n",
      "zipp                     1.0.0\n",
      "zope.interface           5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deb9cb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic           0\n",
       "neutral1        0\n",
       "neutral2     5973\n",
       "neutral3    10065\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get NaN values per column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'neutral1': 'neutral'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d941fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[[\"toxic\", \"neutral\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40670624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = filtered_df.to_dict(orient=\"records\")\n",
    "df_dict = Dataset.from_list(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7087b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify_sentence(sentence: str, max_new_tokens: int = 100) -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f'Detoxify this sentence: \"{sentence}\"'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Format the input using Qwen's chat template\n",
    "    prompt_text = base_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = base_tokenizer([prompt_text], return_tensors=\"pt\").to(base_model.device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = base_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "toxic = \"toxic sentence here\"\n",
    "cleaned = detoxify_sentence(toxic)\n",
    "print(\"Detoxified Output:\", cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d494d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11927/11927 [00:04<00:00, 2921.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    input_text = f\"detoxify: {example['toxic']}\"\n",
    "    target_text = example[\"neutral\"]\n",
    "    input_enc = base_tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=64)\n",
    "    target_enc = base_tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n",
    "    return input_enc\n",
    "\n",
    "tokenized_dataset = df_dict.map(preprocess)\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f89f4",
   "metadata": {},
   "source": [
    "### Whole pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = Dataset.from_pandas(filtered_df)\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(example):\n",
    "    input_text = f\"detoxify: {example['toxic']}\"\n",
    "    target_text = example[\"neutral\"]\n",
    "    input_enc = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=64)\n",
    "    target_enc = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=64)\n",
    "    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n",
    "    return input_enc\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset, eval_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./detoxifier\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# === Trainer ===\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"models/sft-detoxifier\")\n",
    "trainer.tokenizer.save_pretrained(\"models/sft-detoxifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model and tokenizer from models\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_name = \"models/sft-detoxifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify(text, model=model, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(f\"toxic: {text}. neutral: \", return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=120)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(detoxify(\"toxic sentence here\", model=base_model, tokenizer=base_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load detoxifier last checkpoint from models/detoxifier/checkpoint-23500\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_name = \"detoxifier/checkpoint-23500\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f51a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def detoxify(text, model=model, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(f\"toxic: {text}. neutral: \", return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=120)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(detoxify(\"toxic sentence here\", model=model, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fbec1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
