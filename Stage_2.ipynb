{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0b871c-3e59-43aa-bf29-cbb2db782c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "\n",
    "def identity_loss_bertscore(s_orig, s_new):\n",
    "    _, _, F1 = bert_score.score([s_new], [s_orig], lang=\"en\", verbose=False)\n",
    "    return 1.0 - F1[0].item()  # 1 - similarity = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341f918f-1e9f-412f-b211-2371af9a4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def identity_loss_bleu(s_orig, s_new):\n",
    "    ref = s_orig.split()\n",
    "    hyp = s_new.split()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([ref], hyp, smoothing_function=smoothie)\n",
    "    return 1.0 - bleu  # 1 - similarity = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137c3218-99b4-4883-8880-18f634ba757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Loss (BERTScore): 0.1779\n"
     ]
    }
   ],
   "source": [
    "s_orig = \"I love the way you think.\"\n",
    "s_new = \".\"\n",
    "\n",
    "loss = identity_loss_bertscore(s_orig, s_new)\n",
    "print(f\"Identity Loss (BERTScore): {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6929c2a7-1ce8-4300-ba46-2177c1dd77f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               toxic  \\\n",
      "0                           he had steel balls too !   \n",
      "1  dude should have been taken to api , he would ...   \n",
      "2  im not gonna sell the fucking picture , i just...   \n",
      "3  the garbage that is being created by cnn and o...   \n",
      "4  the reason they dont exist is because neither ...   \n",
      "5                i hope they beat each other silly .   \n",
      "6  no good bastards that we are and we are unrepe...   \n",
      "7                 stop the coverage and let em rot .   \n",
      "8                        delete the page and shut up   \n",
      "9         honestly catcalling really sounds stupid .   \n",
      "\n",
      "                                             neutral  bert_score  bleu_score  \n",
      "0                                  he was brave too!    0.914159    0.037019  \n",
      "1  It would have been good if he went to api. He ...    0.897757    0.033800  \n",
      "2  I'm not gonna sell the picture, i just want to...    0.956373    0.463106  \n",
      "3  the news that is being created by cnn and othe...    0.976149    0.721018  \n",
      "4  The reason they don't exist is because neither...    0.948761    0.220377  \n",
      "5                       I think they beat each other    0.923777    0.364093  \n",
      "6  no good people that we are and we are unrepent...    0.963069    0.707662  \n",
      "7                           kindly stop the coverage    0.893393    0.158728  \n",
      "8                             please delete the page    0.884716    0.261699  \n",
      "9                  It's not wise to catcall frankly.    0.891584    0.000000  \n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import bert_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"paradetox.tsv\", sep=\"\\t\")\n",
    "samples = df[['toxic', 'neutral1']].dropna().head(10)\n",
    "\n",
    "results = []\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "for index, row in samples.iterrows():\n",
    "    toxic = row['toxic']\n",
    "    neutral = row['neutral1']\n",
    "\n",
    "    # BERTScore\n",
    "    _, _, F1 = bert_score.score([neutral], [toxic], lang=\"en\", verbose=False)\n",
    "    bert_similarity = F1[0].item()\n",
    "\n",
    "    # BLEU Score\n",
    "    bleu = sentence_bleu([toxic.split()], neutral.split(), smoothing_function=smoothie)\n",
    "\n",
    "    results.append({\n",
    "        'toxic': toxic,\n",
    "        'neutral': neutral,\n",
    "        'bert_score': bert_similarity,\n",
    "        'bleu_score': bleu\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf1ffe-e152-4caf-bc6e-b7bce9a97636",
   "metadata": {},
   "source": [
    "### Analysis on the Results \n",
    "\n",
    "* Bert Scores focus more on the semantic similarity (it is more robust to word-level changes compared to BLEU score)\n",
    "* Good for focusing on word choices.\n",
    "* An idea: focusing on both the structure and the semantics.\n",
    "- BLEU score for word similarity and preservation.\n",
    "- BertScore for meaning similarity and preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106fc2c9-e388-4900-9d35-ba7cba0d903f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86b2538d61a43fca3af55de4dd086b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039a641754be44c3904c5626785d7844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbe61eba0624703bd8ef3048e75be07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d601c7c688412bb6f49d6df8b2d27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3c8efac173477b856f1c3151ddb982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Ensure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Function to embed sentence using [CLS] token\n",
    "def embed_sentence(sentence):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "    return cls_embedding\n",
    "\n",
    "# Differentiable cosine similarity loss (1 - similarity)\n",
    "def identity_loss_cosine(orig_sentence, new_sentence):\n",
    "    emb_orig = embed_sentence(orig_sentence)\n",
    "    emb_new = embed_sentence(new_sentence)\n",
    "    loss = 1.0 - F.cosine_similarity(emb_orig, emb_new).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6109395c-2ff6-4544-b9de-872184332f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Preservation Loss (cosine): 0.3031\n"
     ]
    }
   ],
   "source": [
    "s1 = \"I'm not gonna sell the fucking picture, I just want to show it.\"\n",
    "s2 = \".\"\n",
    "\n",
    "loss = identity_loss_cosine(s1, s2)\n",
    "print(f\"Identity Preservation Loss (cosine): {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71d8ec5-4db6-4d7b-94d9-2684b857bc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               toxic  \\\n",
      "0                           he had steel balls too !   \n",
      "1  dude should have been taken to api , he would ...   \n",
      "2  im not gonna sell the fucking picture , i just...   \n",
      "3  the garbage that is being created by cnn and o...   \n",
      "4  the reason they dont exist is because neither ...   \n",
      "5                i hope they beat each other silly .   \n",
      "6  no good bastards that we are and we are unrepe...   \n",
      "7                 stop the coverage and let em rot .   \n",
      "8                        delete the page and shut up   \n",
      "9         honestly catcalling really sounds stupid .   \n",
      "\n",
      "                                             neutral  cosine_similarity  \\\n",
      "0                                  he was brave too!           0.964783   \n",
      "1  It would have been good if he went to api. He ...           0.918505   \n",
      "2  I'm not gonna sell the picture, i just want to...           0.888744   \n",
      "3  the news that is being created by cnn and othe...           0.987064   \n",
      "4  The reason they don't exist is because neither...           0.826124   \n",
      "5                       I think they beat each other           0.941371   \n",
      "6  no good people that we are and we are unrepent...           0.945258   \n",
      "7                           kindly stop the coverage           0.843993   \n",
      "8                             please delete the page           0.886889   \n",
      "9                  It's not wise to catcall frankly.           0.946762   \n",
      "\n",
      "   identity_loss  \n",
      "0       0.035217  \n",
      "1       0.081495  \n",
      "2       0.111256  \n",
      "3       0.012936  \n",
      "4       0.173876  \n",
      "5       0.058629  \n",
      "6       0.054742  \n",
      "7       0.156007  \n",
      "8       0.113111  \n",
      "9       0.053238  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# Load BERT model + tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"paradetox.tsv\", sep=\"\\t\")\n",
    "samples = df[['toxic', 'neutral1']].dropna().head(10)\n",
    "\n",
    "# Helper: Embed sentence via [CLS]\n",
    "def embed_sentence(sentence):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in samples.iterrows():\n",
    "    toxic = row['toxic']\n",
    "    neutral = row['neutral1']\n",
    "\n",
    "    emb_toxic = embed_sentence(toxic)\n",
    "    emb_neutral = embed_sentence(neutral)\n",
    "\n",
    "    # Cosine similarity → Identity loss\n",
    "    cosine_sim = F.cosine_similarity(emb_toxic, emb_neutral).item()\n",
    "    identity_loss = 1.0 - cosine_sim  # Lower is better\n",
    "\n",
    "    results.append({\n",
    "        'toxic': toxic,\n",
    "        'neutral': neutral,\n",
    "        'cosine_similarity': cosine_sim,\n",
    "        'identity_loss': identity_loss\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05526d88-e159-46e8-8ac9-7a887fbcc18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5040ba16e11c4af1b6ec9920bb818127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717e5d5cb08d464b99a1791a9072195a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarhan/miniconda3/envs/paradetox/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4052: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               toxic  \\\n",
      "0                           he had steel balls too !   \n",
      "1  dude should have been taken to api , he would ...   \n",
      "2  im not gonna sell the fucking picture , i just...   \n",
      "3  the garbage that is being created by cnn and o...   \n",
      "4  the reason they dont exist is because neither ...   \n",
      "5                i hope they beat each other silly .   \n",
      "6  no good bastards that we are and we are unrepe...   \n",
      "7                 stop the coverage and let em rot .   \n",
      "8                        delete the page and shut up   \n",
      "9         honestly catcalling really sounds stupid .   \n",
      "\n",
      "                                             neutral  cross_entropy_loss  \n",
      "0                                  he was brave too!           10.975761  \n",
      "1  It would have been good if he went to api. He ...           10.116467  \n",
      "2  I'm not gonna sell the picture, i just want to...            5.959867  \n",
      "3  the news that is being created by cnn and othe...            4.835916  \n",
      "4  The reason they don't exist is because neither...            9.454285  \n",
      "5                       I think they beat each other            4.962852  \n",
      "6  no good people that we are and we are unrepent...           10.157953  \n",
      "7                           kindly stop the coverage            7.632817  \n",
      "8                             please delete the page            6.457779  \n",
      "9                  It's not wise to catcall frankly.           11.489596  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"paradetox.tsv\", sep=\"\\t\")\n",
    "samples = df[['toxic', 'neutral1']].dropna().head(10)\n",
    "\n",
    "results = []\n",
    "\n",
    "# For each pair, compute cross-entropy from detoxified → original\n",
    "for idx, row in samples.iterrows():\n",
    "    toxic = row['toxic']\n",
    "    neutral = row['neutral1']\n",
    "\n",
    "    # Tokenize both\n",
    "    inputs = tokenizer(neutral, return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(toxic, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # Align length\n",
    "    input_ids = inputs[\"input_ids\"][:, :labels.size(1)]\n",
    "    labels = labels[:, :input_ids.size(1)]\n",
    "\n",
    "    # Compute loss\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, labels=labels)\n",
    "        loss = output.loss.item()\n",
    "\n",
    "    results.append({\n",
    "        'toxic': toxic,\n",
    "        'neutral': neutral,\n",
    "        'cross_entropy_loss': loss\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f10f4454-3fb8-42ed-a265-2df7c5bc0d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarhan/miniconda3/envs/paradetox/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4052: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               toxic  \\\n",
      "0                           he had steel balls too !   \n",
      "1  dude should have been taken to api , he would ...   \n",
      "2  im not gonna sell the fucking picture , i just...   \n",
      "3  the garbage that is being created by cnn and o...   \n",
      "4  the reason they dont exist is because neither ...   \n",
      "5                i hope they beat each other silly .   \n",
      "6  no good bastards that we are and we are unrepe...   \n",
      "7                 stop the coverage and let em rot .   \n",
      "8                        delete the page and shut up   \n",
      "9         honestly catcalling really sounds stupid .   \n",
      "\n",
      "                                             neutral  combined_loss  \n",
      "0                                  he was brave too!       1.449751  \n",
      "1  It would have been good if he went to api. He ...       1.826595  \n",
      "2  I'm not gonna sell the picture, i just want to...       1.708550  \n",
      "3  the news that is being created by cnn and othe...       0.612955  \n",
      "4  The reason they don't exist is because neither...       2.684184  \n",
      "5                       I think they beat each other       1.082574  \n",
      "6  no good people that we are and we are unrepent...       1.563214  \n",
      "7                           kindly stop the coverage       2.323356  \n",
      "8                             please delete the page       1.776890  \n",
      "9                  It's not wise to catcall frankly.       1.681344  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "# Load models\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "embed_model = AutoModel.from_pretrained(\"bert-base-uncased\").eval()\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").eval()\n",
    "\n",
    "# Sentence embedding using [CLS]\n",
    "def embed_sentence(sentence):\n",
    "    tokens = embed_tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = embed_model(**tokens)\n",
    "        return outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "# Combined loss\n",
    "def combined_loss(orig, new, alpha=1.0, beta=1.0):\n",
    "    # Cosine similarity loss\n",
    "    v1 = embed_sentence(orig)\n",
    "    v2 = embed_sentence(new)\n",
    "    cosine_loss = 1.0 - F.cosine_similarity(v1, v2).mean()\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    inputs = gen_tokenizer(new, return_tensors=\"pt\")\n",
    "    with gen_tokenizer.as_target_tokenizer():\n",
    "        labels = gen_tokenizer(orig, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"][:, :labels.size(1)]\n",
    "    labels = labels[:, :input_ids.size(1)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = gen_model(input_ids=input_ids, labels=labels)\n",
    "        cross_entropy = output.loss\n",
    "\n",
    "    return alpha * cosine_loss + beta * cross_entropy/10\n",
    "\n",
    "# Run on first 10 rows\n",
    "df = pd.read_csv(\"paradetox.tsv\", sep=\"\\t\").dropna(subset=['toxic', 'neutral1']).head(10)\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    loss_val = combined_loss(row['toxic'], row['neutral1'], alpha=10.0, beta=1.0)\n",
    "    results.append({\n",
    "        \"toxic\": row[\"toxic\"],\n",
    "        \"neutral\": row[\"neutral1\"],\n",
    "        \"combined_loss\": loss_val.item()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40b155-bc61-47fc-8eba-a8f66d19481d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (paradetox)",
   "language": "python",
   "name": "paradetox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
