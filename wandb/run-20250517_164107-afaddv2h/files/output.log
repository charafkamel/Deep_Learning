  0%|                                                                                                                                                                                          | 0/6900 [00:00<?, ?it/s]/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  2%|████▏                                                                                                                                                                           | 166/6900 [00:37<22:41,  4.95it/s]/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
{'loss': 0.0, 'learning_rate': 0.004, 'epoch': 0.14}
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
  3%|█████                                                                                                                                                                           | 200/6900 [00:45<58:00,  1.93it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.004, 'epoch': 0.29}
  Num examples = 614
  Batch size = 16
  3%|█████                                                                                                                                                                           | 200/6900 [00:46<58:00,  1.93it/s]Saving model checkpoint to outputs/SFT/checkpoint-200
Configuration saved in outputs/SFT/checkpoint-200/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.492, 'eval_samples_per_second': 411.53, 'eval_steps_per_second': 26.14, 'epoch': 0.29}
Model weights saved in outputs/SFT/checkpoint-200/pytorch_model.bin
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  4%|██████▋                                                                                                                                                                         | 261/6900 [01:02<23:13,  4.76it/s]Traceback (most recent call last):
  File "main.py", line 70, in <module>
    main(args)
  File "main.py", line 53, in main
    custom_trainer.trainer.train(resume_from_checkpoint=False)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py", line 1521, in train
    return inner_training_loop(
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py", line 1763, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py", line 2499, in training_step
    loss = self.compute_loss(model, inputs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py", line 2531, in compute_loss
    outputs = model(**inputs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 1648, in forward
    decoder_outputs = self.decoder(
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 1040, in forward
    layer_outputs = layer_module(
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 725, in forward
    hidden_states = self.layer[-1](hidden_states)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 328, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 310, in forward
    hidden_states = self.dropout(hidden_states)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/functional.py", line 1168, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
KeyboardInterrupt
