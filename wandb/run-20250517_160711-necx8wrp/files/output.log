  0%|                                                                                                                                                                                           | 0/6900 [00:00<?, ?it/s]/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  2%|████▎                                                                                                                                                                            | 166/6900 [00:37<23:48,  4.72it/s]/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 0.14}
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
  3%|█████▏                                                                                                                                                                           | 200/6900 [00:45<58:51,  1.90it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 0.29}
  Num examples = 614
  Batch size = 16
  3%|█████▏                                                                                                                                                                           | 200/6900 [00:46<58:51,  1.90it/s]Saving model checkpoint to outputs/SFT/checkpoint-200
Configuration saved in outputs/SFT/checkpoint-200/config.json                                                                                                                                                            
{'eval_loss': nan, 'eval_runtime': 1.5185, 'eval_samples_per_second': 404.345, 'eval_steps_per_second': 25.683, 'epoch': 0.29}
Model weights saved in outputs/SFT/checkpoint-200/pytorch_model.bin
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  6%|██████████▎                                                                                                                                                                      | 400/6900 [01:33<57:01,  1.90it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 0.43}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 0.58}
  Num examples = 614
  Batch size = 16
  6%|██████████▎                                                                                                                                                                      | 400/6900 [01:35<57:01,  1.90it/s]Saving model checkpoint to outputs/SFT/checkpoint-400
Configuration saved in outputs/SFT/checkpoint-400/config.json                                                                                                                                                            
{'eval_loss': nan, 'eval_runtime': 1.522, 'eval_samples_per_second': 403.406, 'eval_steps_per_second': 25.623, 'epoch': 0.58}
Model weights saved in outputs/SFT/checkpoint-400/pytorch_model.bin
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  9%|███████████████▍                                                                                                                                                                 | 600/6900 [02:22<55:18,  1.90it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 0.72}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 0.87}
  Num examples = 614
  Batch size = 16
  9%|███████████████▍                                                                                                                                                                 | 600/6900 [02:24<55:18,  1.90it/s]Saving model checkpoint to outputs/SFT/checkpoint-600
Configuration saved in outputs/SFT/checkpoint-600/config.json                                                                                                                                                            
{'eval_loss': nan, 'eval_runtime': 1.5113, 'eval_samples_per_second': 406.271, 'eval_steps_per_second': 25.805, 'epoch': 0.87}
Model weights saved in outputs/SFT/checkpoint-600/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-400] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 12%|████████████████████▌                                                                                                                                                            | 800/6900 [03:11<55:40,  1.83it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 1.01}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 1.16}
  Num examples = 614
  Batch size = 16
 12%|████████████████████▌                                                                                                                                                            | 800/6900 [03:13<55:40,  1.83it/s]Saving model checkpoint to outputs/SFT/checkpoint-800
Configuration saved in outputs/SFT/checkpoint-800/config.json                                                                                                                                                            
{'eval_loss': nan, 'eval_runtime': 1.4332, 'eval_samples_per_second': 428.425, 'eval_steps_per_second': 27.213, 'epoch': 1.16}
Model weights saved in outputs/SFT/checkpoint-800/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-600] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 14%|█████████████████████████▌                                                                                                                                                      | 1000/6900 [04:00<52:44,  1.86it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 1.3}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 1.45}
  Num examples = 614
  Batch size = 16
 14%|█████████████████████████▌                                                                                                                                                      | 1000/6900 [04:01<52:44,  1.86it/s]Saving model checkpoint to outputs/SFT/checkpoint-1000
Configuration saved in outputs/SFT/checkpoint-1000/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.4401, 'eval_samples_per_second': 426.365, 'eval_steps_per_second': 27.082, 'epoch': 1.45}
Model weights saved in outputs/SFT/checkpoint-1000/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-800] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 17%|██████████████████████████████▌                                                                                                                                                 | 1200/6900 [04:49<50:18,  1.89it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 1.59}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 1.74}
  Num examples = 614
  Batch size = 16
 17%|██████████████████████████████▌                                                                                                                                                 | 1200/6900 [04:50<50:18,  1.89it/s]Saving model checkpoint to outputs/SFT/checkpoint-1200
Configuration saved in outputs/SFT/checkpoint-1200/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.4537, 'eval_samples_per_second': 422.365, 'eval_steps_per_second': 26.828, 'epoch': 1.74}
Model weights saved in outputs/SFT/checkpoint-1200/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-1000] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 20%|███████████████████████████████████▋                                                                                                                                            | 1400/6900 [05:31<32:38,  2.81it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 1.88}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 2.03}
  Num examples = 614
  Batch size = 16
 20%|███████████████████████████████████▋                                                                                                                                            | 1400/6900 [05:32<32:38,  2.81it/s]Saving model checkpoint to outputs/SFT/checkpoint-1400
Configuration saved in outputs/SFT/checkpoint-1400/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 0.993, 'eval_samples_per_second': 618.349, 'eval_steps_per_second': 39.276, 'epoch': 2.03}
Model weights saved in outputs/SFT/checkpoint-1400/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-1200] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 23%|████████████████████████████████████████▊                                                                                                                                       | 1600/6900 [06:20<48:35,  1.82it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 2.17}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 2.32}
  Num examples = 614
  Batch size = 16
 23%|████████████████████████████████████████▊                                                                                                                                       | 1600/6900 [06:21<48:35,  1.82it/s]Saving model checkpoint to outputs/SFT/checkpoint-1600
Configuration saved in outputs/SFT/checkpoint-1600/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5146, 'eval_samples_per_second': 405.386, 'eval_steps_per_second': 25.749, 'epoch': 2.32}
Model weights saved in outputs/SFT/checkpoint-1600/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-1400] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 26%|█████████████████████████████████████████████▉                                                                                                                                  | 1800/6900 [07:09<45:18,  1.88it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 2.46}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 2.61}
  Num examples = 614
  Batch size = 16
 26%|█████████████████████████████████████████████▉                                                                                                                                  | 1800/6900 [07:10<45:18,  1.88it/s]Saving model checkpoint to outputs/SFT/checkpoint-1800
Configuration saved in outputs/SFT/checkpoint-1800/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5159, 'eval_samples_per_second': 405.043, 'eval_steps_per_second': 25.727, 'epoch': 2.61}
Model weights saved in outputs/SFT/checkpoint-1800/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-1600] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 29%|███████████████████████████████████████████████████                                                                                                                             | 2000/6900 [07:58<43:46,  1.87it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 2.75}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 2.9}
  Num examples = 614
  Batch size = 16
 29%|███████████████████████████████████████████████████                                                                                                                             | 2000/6900 [08:00<43:46,  1.87it/s]Saving model checkpoint to outputs/SFT/checkpoint-2000
Configuration saved in outputs/SFT/checkpoint-2000/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5123, 'eval_samples_per_second': 406.011, 'eval_steps_per_second': 25.789, 'epoch': 2.9}
Model weights saved in outputs/SFT/checkpoint-2000/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-1800] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 32%|████████████████████████████████████████████████████████                                                                                                                        | 2200/6900 [08:47<41:54,  1.87it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 3.04}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 3.19}
  Num examples = 614
  Batch size = 16
 32%|████████████████████████████████████████████████████████                                                                                                                        | 2200/6900 [08:48<41:54,  1.87it/s]Saving model checkpoint to outputs/SFT/checkpoint-2200
Configuration saved in outputs/SFT/checkpoint-2200/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5175, 'eval_samples_per_second': 404.607, 'eval_steps_per_second': 25.7, 'epoch': 3.19}
Model weights saved in outputs/SFT/checkpoint-2200/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-2000] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 35%|█████████████████████████████████████████████████████████████▏                                                                                                                  | 2400/6900 [09:36<39:09,  1.91it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 3.33}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 3.48}
  Num examples = 614
  Batch size = 16
 35%|█████████████████████████████████████████████████████████████▏                                                                                                                  | 2400/6900 [09:37<39:09,  1.91it/s]Saving model checkpoint to outputs/SFT/checkpoint-2400
Configuration saved in outputs/SFT/checkpoint-2400/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5068, 'eval_samples_per_second': 407.487, 'eval_steps_per_second': 25.883, 'epoch': 3.48}
Model weights saved in outputs/SFT/checkpoint-2400/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-2200] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 38%|██████████████████████████████████████████████████████████████████▎                                                                                                             | 2600/6900 [10:24<39:20,  1.82it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 3.62}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 3.77}
  Num examples = 614
  Batch size = 16
 38%|██████████████████████████████████████████████████████████████████▎                                                                                                             | 2600/6900 [10:26<39:20,  1.82it/s]Saving model checkpoint to outputs/SFT/checkpoint-2600
Configuration saved in outputs/SFT/checkpoint-2600/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.4977, 'eval_samples_per_second': 409.952, 'eval_steps_per_second': 26.039, 'epoch': 3.77}
Model weights saved in outputs/SFT/checkpoint-2600/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-2400] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 41%|███████████████████████████████████████████████████████████████████████▍                                                                                                        | 2800/6900 [11:13<35:50,  1.91it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 3.91}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 4.06}
  Num examples = 614
  Batch size = 16
 41%|███████████████████████████████████████████████████████████████████████▍                                                                                                        | 2800/6900 [11:14<35:50,  1.91it/s]Saving model checkpoint to outputs/SFT/checkpoint-2800
Configuration saved in outputs/SFT/checkpoint-2800/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.4991, 'eval_samples_per_second': 409.57, 'eval_steps_per_second': 26.015, 'epoch': 4.06}
Model weights saved in outputs/SFT/checkpoint-2800/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-2600] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 43%|████████████████████████████████████████████████████████████████████████████▌                                                                                                   | 3000/6900 [12:01<34:46,  1.87it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 4.2}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 4.35}
  Num examples = 614
  Batch size = 16
 43%|████████████████████████████████████████████████████████████████████████████▌                                                                                                   | 3000/6900 [12:03<34:46,  1.87it/s]Saving model checkpoint to outputs/SFT/checkpoint-3000
Configuration saved in outputs/SFT/checkpoint-3000/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5005, 'eval_samples_per_second': 409.186, 'eval_steps_per_second': 25.991, 'epoch': 4.35}
Model weights saved in outputs/SFT/checkpoint-3000/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-2800] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 46%|█████████████████████████████████████████████████████████████████████████████████▌                                                                                              | 3200/6900 [12:50<32:25,  1.90it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 4.49}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 4.64}
  Num examples = 614
  Batch size = 16
 46%|█████████████████████████████████████████████████████████████████████████████████▌                                                                                              | 3200/6900 [12:52<32:25,  1.90it/s]Saving model checkpoint to outputs/SFT/checkpoint-3200
Configuration saved in outputs/SFT/checkpoint-3200/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5159, 'eval_samples_per_second': 405.051, 'eval_steps_per_second': 25.728, 'epoch': 4.64}
Model weights saved in outputs/SFT/checkpoint-3200/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-3000] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 49%|██████████████████████████████████████████████████████████████████████████████████████▋                                                                                         | 3400/6900 [13:39<31:03,  1.88it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 4.78}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 4.93}
  Num examples = 614
  Batch size = 16
 49%|██████████████████████████████████████████████████████████████████████████████████████▋                                                                                         | 3400/6900 [13:40<31:03,  1.88it/s]Saving model checkpoint to outputs/SFT/checkpoint-3400
Configuration saved in outputs/SFT/checkpoint-3400/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5173, 'eval_samples_per_second': 404.667, 'eval_steps_per_second': 25.704, 'epoch': 4.93}
Model weights saved in outputs/SFT/checkpoint-3400/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-3200] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 52%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                                                    | 3600/6900 [14:27<29:16,  1.88it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 5.07}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 5.22}
  Num examples = 614
  Batch size = 16
 52%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                                                    | 3600/6900 [14:29<29:16,  1.88it/s]Saving model checkpoint to outputs/SFT/checkpoint-3600
Configuration saved in outputs/SFT/checkpoint-3600/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5019, 'eval_samples_per_second': 408.815, 'eval_steps_per_second': 25.967, 'epoch': 5.22}
Model weights saved in outputs/SFT/checkpoint-3600/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-3400] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 55%|████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                               | 3800/6900 [15:16<28:08,  1.84it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 5.36}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 5.51}
  Num examples = 614
  Batch size = 16
 55%|████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                               | 3800/6900 [15:18<28:08,  1.84it/s]Saving model checkpoint to outputs/SFT/checkpoint-3800
Configuration saved in outputs/SFT/checkpoint-3800/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5122, 'eval_samples_per_second': 406.038, 'eval_steps_per_second': 25.791, 'epoch': 5.51}
Model weights saved in outputs/SFT/checkpoint-3800/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-3600] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 58%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                          | 4000/6900 [16:05<27:05,  1.78it/s]***** Running Evaluation *****
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 5.65}
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 5.8}
  Num examples = 614
  Batch size = 16
 58%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                          | 4000/6900 [16:07<27:05,  1.78it/s]Saving model checkpoint to outputs/SFT/checkpoint-4000
Configuration saved in outputs/SFT/checkpoint-4000/config.json                                                                                                                                                           
{'eval_loss': nan, 'eval_runtime': 1.5176, 'eval_samples_per_second': 404.577, 'eval_steps_per_second': 25.698, 'epoch': 5.8}
Model weights saved in outputs/SFT/checkpoint-4000/pytorch_model.bin
Deleting older checkpoint [outputs/SFT/checkpoint-3800] due to args.save_total_limit
/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                     | 4161/6900 [16:45<09:34,  4.77it/s]Traceback (most recent call last):
{'loss': 0.0, 'learning_rate': 0.0004, 'epoch': 5.94}
  File "main.py", line 59, in <module>
    main(args)
  File "main.py", line 42, in main
    custom_trainer.trainer.train(resume_from_checkpoint=False)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py", line 1521, in train
    return inner_training_loop(
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/transformers/trainer.py", line 1808, in _inner_training_loop
    nn.utils.clip_grad_norm_(
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py", line 43, in clip_grad_norm_
    if total_norm.isnan() or total_norm.isinf():
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/threading.py", line 1011, in join
    self._wait_for_tstate_lock()
  File "/scratch/students/2025-spring-sp-tarhan/miniconda3/envs/detox/lib/python3.8/threading.py", line 1027, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
