{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfdcd56-2145-4012-93a4-9c86a962915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e3bf99-c7f3-43cb-a919-edecda5e343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data/paradox.tsv file as pandas dataframe\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, 'data')\n",
    "df = pd.read_csv(current_dir +'/paradetox.tsv', sep='\\t')\n",
    "\n",
    "## rename neutral1 column to neutral\n",
    "df.rename(columns={'neutral1': 'neutral'}, inplace=True)\n",
    "\n",
    "filtered_df = df[[\"toxic\", \"neutral\"]]\n",
    "df_dict = filtered_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b393ef-5259-453c-85c5-fb4f17e3c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10734\n",
      "1193\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "# Step 1: Convert to dict of lists\n",
    "dict_of_lists = {\n",
    "    'toxic': [entry['toxic'] for entry in df_dict],\n",
    "    'neutral': [entry['neutral'] for entry in df_dict]\n",
    "}\n",
    "\n",
    "# Step 2: Create Hugging Face Dataset\n",
    "dataset = Dataset.from_dict(dict_of_lists)\n",
    "\n",
    "split = dataset.train_test_split(test_size=0.1, seed = 42)\n",
    "dataset, test_dataset = split[\"train\"], split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aa11eac-99e1-405b-8503-f082cb97a054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {0: 'toxic', 1: 'severe_toxic', 2: 'obscene', 3: 'threat', 4: 'insult', 5: 'identity_hate'}\n",
      "Original probs: {'toxic': 0.9989174604415894, 'severe_toxic': 0.6191971302032471, 'insult': 0.9701922535896301}\n",
      " remove          i → toxic Δ=-0.0000\n",
      " remove       hate → severe_toxic Δ=0.0855\n",
      " remove       your → severe_toxic Δ=0.1543\n",
      " remove    fucking → severe_toxic Δ=0.2032\n",
      " remove      black → severe_toxic Δ=0.2380\n",
      " remove        ass → severe_toxic Δ=0.0202\n",
      " remove          ! → toxic Δ=-0.0002\n",
      "Detected toxic markers: [('hate', 'severe_toxic', 0.0855), ('your', 'severe_toxic', 0.1543), ('fucking', 'severe_toxic', 0.2032), ('black', 'severe_toxic', 0.238), ('ass', 'severe_toxic', 0.0202)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"unitary/toxic-bert\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model      = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# map indices → labels\n",
    "id2label = model.config.id2label\n",
    "print(\"Label mapping:\", id2label)\n",
    "# e.g. {0:'toxic', 1:'severe_toxic', 2:'obscene', 3:'threat', 4:'insult', 5:'identity_hate'}\n",
    "\n",
    "# choose which category (or categories) you care about\n",
    "# here we’ll do both: 0=\"toxic\" and 4=\"insult\"\n",
    "target_idxs = {\n",
    "    \"toxic\":  0,\n",
    "    \"severe_toxic\" : 1,\n",
    "    \"insult\": 4,\n",
    "}\n",
    "\n",
    "def get_probs(text: str, device=\"cpu\"):\n",
    "    \"\"\"Run the model and return a tensor of shape (num_labels,) with sigmoid-ed probs.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits.squeeze(0)        # (6,)\n",
    "    return torch.sigmoid(logits)                        # (6,)\n",
    "\n",
    "def detect_toxic_tokens(text: str,\n",
    "                        threshold: float = 0.02,\n",
    "                        device: str = None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 1) original per‐label probs\n",
    "    orig_probs = get_probs(text, device)\n",
    "    print(\"Original probs:\", {lab: orig_probs[idx].item() \n",
    "                               for lab,idx in target_idxs.items()})\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    markers = []\n",
    "\n",
    "    # 2) leave-one-out\n",
    "    for i, tok in enumerate(tokens):\n",
    "        # remove the i-th token\n",
    "        reduced = tokens[:i] + tokens[i+1:]\n",
    "        new_text = tokenizer.convert_tokens_to_string(reduced)\n",
    "\n",
    "        probs_m = get_probs(new_text, device)\n",
    "        deltas = {lab: orig_probs[idx].item() - probs_m[idx].item()\n",
    "                  for lab,idx in target_idxs.items()}\n",
    "\n",
    "        # pick the *maximum* drop across your targets\n",
    "        best_lab, best_delta = max(deltas.items(), key=lambda kv: kv[1])\n",
    "        print(f\" remove {tok:>10} → {best_lab} Δ={best_delta:.4f}\")\n",
    "\n",
    "        if best_delta >= threshold:\n",
    "            markers.append((tok, best_lab, round(best_delta,4)))\n",
    "\n",
    "    return markers\n",
    "\n",
    "# Example\n",
    "sentence = \"I hate your fucking black ass!\"\n",
    "markers  = detect_toxic_tokens(sentence, threshold=0.02)\n",
    "print(\"Detected toxic markers:\", markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4454bab9-0eda-476d-ae81-432e6eeb83a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most toxic input: I hate your fucking black ass!\n",
      "Masking 'black' (label=severe_toxic, Δ=0.2380)\n",
      "🔒 Masked:    I hate your fucking [MASK] ass!\n",
      "✍️  Filled:    i hate your fucking smart ass!\n",
      "Masking 'your' (label=insult, Δ=0.2907)\n",
      "🔒 Masked:    i hate [MASK] fucking smart ass!\n",
      "✍️  Filled:    i hate my fucking smart ass!\n",
      "Masking 'fucking' (label=insult, Δ=0.2637)\n",
      "🔒 Masked:    i hate my [MASK] smart ass!\n",
      "✍️  Filled:    i hate my old smart ass!\n",
      "Masking 'ass' (label=toxic, Δ=0.6321)\n",
      "🔒 Masked:    i hate my old smart [MASK]!\n",
      "✍️  Filled:    i hate my old smarts!\n",
      "Masking 'hate' (label=toxic, Δ=0.2605)\n",
      "🔒 Masked:    i [MASK] my old smarts!\n",
      "✍️  Filled:    i like my old smarts!\n",
      "\n",
      "Rewrite steps:\n",
      "  0. I hate your fucking black ass!\n",
      "  1. i hate your fucking smart ass!\n",
      "  2. i hate my fucking smart ass!\n",
      "  3. i hate my old smart ass!\n",
      "  4. i hate my old smarts!\n",
      "  5. i like my old smarts!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ── 1) Load detector & fill-mask ─────────────────────────────────────────\n",
    "detector_name = \"unitary/toxic-bert\"\n",
    "tokenizer     = AutoTokenizer.from_pretrained(detector_name)\n",
    "detector      = AutoModelForSequenceClassification.from_pretrained(detector_name).eval()\n",
    "\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model     = \"bert-base-uncased\",\n",
    "    tokenizer = tokenizer,\n",
    "    device    = device_id\n",
    ")\n",
    "\n",
    "# multi-label indices we care about\n",
    "target_idxs = {\"toxic\": 0, \"severe_toxic\": 1, \"insult\": 4}\n",
    "\n",
    "def get_sigmoid_probs(text: str) -> torch.Tensor:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = detector(**inputs).logits.squeeze(0)\n",
    "    return torch.sigmoid(logits)  # shape (6,)\n",
    "\n",
    "def detect_toxic_tokens(text: str, threshold: float = 0.02):\n",
    "    \"\"\"\n",
    "    Leave-one-out Δ on each target label; return all (tok, label, Δ) ≥ threshold.\n",
    "    \"\"\"\n",
    "    orig   = get_sigmoid_probs(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    markers = []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        reduced  = tokens[:i] + tokens[i+1:]\n",
    "        new_text = tokenizer.convert_tokens_to_string(reduced)\n",
    "        probs_m  = get_sigmoid_probs(new_text)\n",
    "        deltas   = {\n",
    "            lab: orig[idx].item() - probs_m[idx].item()\n",
    "            for lab, idx in target_idxs.items()\n",
    "        }\n",
    "        best_lab, best_delta = max(deltas.items(), key=lambda kv: kv[1])\n",
    "        if best_delta >= threshold:\n",
    "            markers.append((tok, best_lab, best_delta))\n",
    "    return markers\n",
    "\n",
    "def rewrite_one_by_one(text: str, threshold: float = 0.02, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Iteratively:\n",
    "      1. detect all toxic subwords\n",
    "      2. pick the one with the highest Δ\n",
    "      3. mask its first occurrence\n",
    "      4. fill with top_k candidates and pick the one with lowest P(toxic)\n",
    "      until no Δ ≥ threshold remains.\n",
    "    Returns the chain of rewrites.\n",
    "    \"\"\"\n",
    "    history = [text]\n",
    "    while True:\n",
    "        # 1) detect\n",
    "        markers = detect_toxic_tokens(text, threshold)\n",
    "        if not markers:\n",
    "            break\n",
    "\n",
    "        # 2) choose single worst token\n",
    "        tok, lab, delta = max(markers, key=lambda x: x[2])\n",
    "        print(f\"Masking '{tok}' (label={lab}, Δ={delta:.4f})\")\n",
    "\n",
    "        # 3) mask only first occurrence\n",
    "        pat    = re.compile(r\"\\b\" + re.escape(tok) + r\"\\b\", flags=re.IGNORECASE)\n",
    "        masked, _ = pat.subn(tokenizer.mask_token, text, count=1)\n",
    "        print(\"🔒 Masked:   \", masked)\n",
    "\n",
    "        # 4) fill and pick best detoxified synonym\n",
    "        candidates = fill_mask(masked, top_k=top_k)\n",
    "        best_seq   = None\n",
    "        best_score = float(\"inf\")\n",
    "\n",
    "        for cand in candidates:\n",
    "            seq        = cand[\"sequence\"]\n",
    "            # skip if it simply re-inserts the same toxic token\n",
    "            if tok.lower() in cand[\"token_str\"].lower():\n",
    "                continue\n",
    "            # score toxicity of the filled sequence\n",
    "            score = get_sigmoid_probs(seq)[ target_idxs[\"toxic\"] ].item()\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_seq   = seq\n",
    "\n",
    "        # fallback: if no suitable candidate, just remove the mask\n",
    "        if best_seq is None:\n",
    "            best_seq = masked.replace(tokenizer.mask_token, \"\", 1)\n",
    "\n",
    "        print(\"✍️  Filled:   \", best_seq)\n",
    "        text = best_seq\n",
    "        history.append(text)\n",
    "\n",
    "    return history\n",
    "\n",
    "# ── Example ─────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    sentences = [\n",
    "        \"I hate your fucking black ass!\",\n",
    "        \"You're such an idiot and a total disgrace!\",\n",
    "        \"Go back to where you came from!\",\n",
    "    ]\n",
    "    # pick most toxic by original P(toxic)\n",
    "    scored     = [(s, get_sigmoid_probs(s)[0].item()) for s in sentences]\n",
    "    most_toxic = max(scored, key=lambda x: x[1])[0]\n",
    "    print(\"Most toxic input:\", most_toxic)\n",
    "\n",
    "    chain = rewrite_one_by_one(most_toxic, threshold=0.02, top_k=10)\n",
    "    print(\"\\nRewrite steps:\")\n",
    "    for i, step in enumerate(chain):\n",
    "        print(f\" {i:>2d}. {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c769ef43-a231-44eb-bda8-bc159b1c4562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: I hate your fucking black ass!\n",
      "Back-translation candidates (P(toxic)):\n",
      " 0.9989 → I hate your fucking black ass!\n",
      " 0.9989 → I hate your fucking black ass!\n",
      " 0.9989 → I hate your fucking black ass!\n",
      " 0.9989 → I hate your fucking black ass!\n",
      " 0.9989 → I hate your fucking black ass!\n",
      " 0.9989 → I hate your fucking black ass!\n",
      " 0.9989 → I hate your fucking black ass!\n",
      " 0.9989 → I hate your fucking black ass!\n",
      "\n",
      "Chosen detoxified: (0.9989) I hate your fucking black ass!\n",
      "Detoxified: I hate your fucking black ass!\n",
      "\n",
      "Original: You're such an idiot and a total disgrace!\n",
      "Back-translation candidates (P(toxic)):\n",
      " 0.9902 → You're an idiot and a disgrace!\n",
      " 0.9899 → You're an idiot and a total shame!\n",
      " 0.9901 → You're such an idiot and a disgrace!\n",
      " 0.9901 → You're such an idiot and a disgrace!\n",
      " 0.9899 → You're an idiot and a total shame!\n",
      " 0.9901 → You're such an idiot and a disgrace!\n",
      " 0.9899 → You're an idiot and a total shame!\n",
      " 0.9899 → You're an idiot and a total shame!\n",
      "\n",
      "Chosen detoxified: (0.9899) You're an idiot and a total shame!\n",
      "Detoxified: You're an idiot and a total shame!\n",
      "\n",
      "Original: Go back to where you came from!\n",
      "Back-translation candidates (P(toxic)):\n",
      " 0.0631 → Go back to where you came from!\n",
      " 0.0631 → Go back to where you came from!\n",
      " 0.0631 → Go back to where you came from!\n",
      " 0.0631 → Go back to where you came from!\n",
      " 0.0631 → Go back to where you came from!\n",
      " 0.0631 → Go back to where you came from!\n",
      " 0.0631 → Go back to where you came from!\n",
      " 0.0631 → Go back to where you came from!\n",
      "\n",
      "Chosen detoxified: (0.0631) Go back to where you came from!\n",
      "Detoxified: Go back to where you came from!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    MarianMTModel, MarianTokenizer,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "# ── Device setup ─────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ── 1) Load the toxicity detector ────────────────────────────────────────\n",
    "detector_name      = \"unitary/toxic-bert\"\n",
    "detector_tokenizer = AutoTokenizer.from_pretrained(detector_name)\n",
    "detector_model     = AutoModelForSequenceClassification.from_pretrained(detector_name).to(device).eval()\n",
    "\n",
    "def get_sigmoid_probs(text: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a (6,) tensor of independent sigmoid probabilities\n",
    "    for the labels [toxic, severe_toxic, obscene, threat, insult, identity_hate].\n",
    "    \"\"\"\n",
    "    inputs = detector_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = detector_model(**inputs).logits.squeeze(0)\n",
    "    return torch.sigmoid(logits)\n",
    "\n",
    "# ── 2) Load the back-translation models ─────────────────────────────────\n",
    "# English → French\n",
    "en_fr_name      = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "en_fr_tokenizer = MarianTokenizer.from_pretrained(en_fr_name)\n",
    "en_fr_model     = MarianMTModel.from_pretrained(en_fr_name).to(device).eval()\n",
    "\n",
    "# French → English\n",
    "fr_en_name      = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "fr_en_tokenizer = MarianTokenizer.from_pretrained(fr_en_name)\n",
    "fr_en_model     = MarianMTModel.from_pretrained(fr_en_name).to(device).eval()\n",
    "\n",
    "# ── 3) Back-translation with toxicity scoring ───────────────────────────\n",
    "def back_translate_detox(\n",
    "    text: str,\n",
    "    num_samples: int = 8,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.95\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    1) Sample `num_samples` English→French translations (stochastic).\n",
    "    2) For each French, sample one French→English back-translation.\n",
    "    3) Score each back-translated English with P(toxic).\n",
    "    4) Return the candidate with the lowest toxicity score.\n",
    "    \"\"\"\n",
    "    # 1) English → French samples\n",
    "    en_fr_inputs = en_fr_tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    fr_variants = []\n",
    "    for _ in range(num_samples):\n",
    "        fr_out = en_fr_model.generate(\n",
    "            **en_fr_inputs,\n",
    "            max_length=128,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        fr_text = en_fr_tokenizer.decode(fr_out[0], skip_special_tokens=True)\n",
    "        fr_variants.append(fr_text)\n",
    "\n",
    "    # 2) French → English back-translations\n",
    "    eng_candidates = []\n",
    "    for fr in fr_variants:\n",
    "        fr_en_inputs = fr_en_tokenizer(fr, return_tensors=\"pt\", padding=True).to(device)\n",
    "        en_out = fr_en_model.generate(\n",
    "            **fr_en_inputs,\n",
    "            max_length=128,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        en_text = fr_en_tokenizer.decode(en_out[0], skip_special_tokens=True)\n",
    "        eng_candidates.append(en_text)\n",
    "\n",
    "    # 3) Score each candidate and pick lowest P(toxic)\n",
    "    scored = []\n",
    "    for cand in eng_candidates:\n",
    "        score = get_sigmoid_probs(cand)[0].item()  # index 0 = \"toxic\"\n",
    "        scored.append((cand, score))\n",
    "\n",
    "    print(\"Back-translation candidates (P(toxic)):\")\n",
    "    for cand, score in scored:\n",
    "        print(f\" {score:.4f} → {cand}\")\n",
    "\n",
    "    best, best_score = min(scored, key=lambda x: x[1])\n",
    "    print(f\"\\nChosen detoxified: ({best_score:.4f}) {best}\")\n",
    "    return best\n",
    "\n",
    "# ── 4) Example usage ─────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    toxic_sentences = [\n",
    "        \"I hate your fucking black ass!\",\n",
    "        \"You're such an idiot and a total disgrace!\",\n",
    "        \"Go back to where you came from!\"\n",
    "    ]\n",
    "    for sent in toxic_sentences:\n",
    "        print(\"\\nOriginal:\", sent)\n",
    "        detox = back_translate_detox(sent, num_samples=8, top_k=50, top_p=0.95)\n",
    "        print(\"Detoxified:\", detox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8219f4-729f-4c13-baba-3bdb78d876da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"paradetox.tsv\", sep=\"\\t\").rename(columns={'neutral1':'neutral'})\n",
    "train, valid = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "for split, subset in ((\"train\",train),(\"valid\",valid)):\n",
    "    with open(f\"DiffuSeq/datasets/detox/{split}.jsonl\",\"w\") as fout:\n",
    "        for _, row in subset.iterrows():\n",
    "            rec = {\"src\": row[\"toxic\"], \"trg\": row[\"neutral\"]}\n",
    "            fout.write(json.dumps(rec, ensure_ascii=False)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbaed4d-30dd-4864-a056-f1e50f2ef1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
